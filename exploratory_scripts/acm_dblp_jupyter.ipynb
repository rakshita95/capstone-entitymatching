{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACM-DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script calls functions from modules and completes a whole run for the google amazon dataset\n",
    "\"\"\"\n",
    "\n",
    "## Preprocess any specific columns (e.g. price column for differnt currency)\n",
    "## Then get the three types of matrices for both google and amazon data (6 matrices in total)\n",
    "## Call similarity functions on 3 pairs of matrices\n",
    "## Concatenate previous results to form the final dataset for modeling\n",
    "## Call modeling functions (train test split etc)\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('/anaconda/lib/python3.6/site-packages')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from modules.preprocessing import Preprocessing\n",
    "from modules.preprocessing.generate_labels import gen_labels\n",
    "from modules.feature_generation.gen_similarities import similarities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from modules.preprocessing import Preprocessor\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve,  precision_score, recall_score, f1_score\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on sample data to find best model using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read data\n",
    "'''\n",
    "#df1 = pd.read_csv(\"data/acm_dblp/full/ACM.csv\", engine='python')\n",
    "#df2 = pd.read_csv(\"data/acm_dblp/full/DBLP2.csv\", engine='python')\n",
    "#match_df = pd.read_csv(\"data/acm_dblp/full/DBLP-ACM_perfectMapping.csv\")\n",
    "\n",
    "df1 = pd.read_csv(\"data/acm_dblp/sample/acm_sample.csv\")\n",
    "df2 = pd.read_csv(\"data/acm_dblp/sample/dblp_sample.csv\")\n",
    "match_df = pd.read_csv(\"data/acm_dblp/sample/acm_dblp_sample_match.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdf1 train shape:  (125, 4) \n",
      " \tmatch train shape:  (110, 2) \n",
      "\tdf1 test shape:  (63, 4) \n",
      "\tmatch test shape:  (53, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "specify id names\n",
    "'''\n",
    "df1_id = 'id'\n",
    "df2_id = 'id'\n",
    "match_id1 = 'idACM' # corresponds to df1_id\n",
    "match_id2 = 'idDBLP' # corresponds to df2_id\n",
    "\n",
    "'''\n",
    "train/test split on input dataset\n",
    "'''\n",
    "#random split inputs into train/test using original dataset\n",
    "df1_train, df1_test = train_test_split(df1, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "#set index dic\n",
    "df1_train_index = dict(zip(df1_train[df1_id], df1_train.reset_index().index))\n",
    "df1_test_index = dict(zip(df1_test[df1_id], df1_test.reset_index().index))\n",
    "\n",
    "'''\n",
    "id column manipulation\n",
    "'''\n",
    "# save for later use to generate labels\n",
    "df1_train_id_col = df1_train[df1_id]\n",
    "df1_test_id_col = df1_test[df1_id]\n",
    "df2_id_col = df2[df2_id]\n",
    "\n",
    "match_train = match_df[match_df[match_id1].isin(df1_train_id_col)]\n",
    "match_test = match_df[match_df[match_id1].isin(df1_test_id_col)]\n",
    "\n",
    "#drop id columns because we don't need to compute id similarity\n",
    "df1_train = df1_train.drop(columns = [df1_id])\n",
    "df1_test = df1_test.drop(columns = [df1_id])\n",
    "df2 = df2.drop(columns = [df2_id])\n",
    "\n",
    "print('\\tdf1 train shape: ', df1_train.shape, '\\n',\n",
    "      '\\tmatch train shape: ', match_train.shape, '\\n'\n",
    "      '\\tdf1 test shape: ', df1_test.shape, '\\n'\n",
    "      '\\tmatch test shape: ', match_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idDBLP</th>\n",
       "      <th>idACM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>conf/sigmod/HsiaoN00</td>\n",
       "      <td>335461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conf/sigmod/SaltenisJLL00</td>\n",
       "      <td>335427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>conf/sigmod/Mohan99</td>\n",
       "      <td>304230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>conf/sigmod/LakshmananNHP99</td>\n",
       "      <td>304196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>conf/sigmod/KanthRSB99</td>\n",
       "      <td>304240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         idDBLP   idACM\n",
       "7          conf/sigmod/HsiaoN00  335461\n",
       "8     conf/sigmod/SaltenisJLL00  335427\n",
       "12          conf/sigmod/Mohan99  304230\n",
       "16  conf/sigmod/LakshmananNHP99  304196\n",
       "17       conf/sigmod/KanthRSB99  304240"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***preprocessing***\n",
      "**** df1 divide columns ****\n",
      "numerical_cols :  ['year']\n",
      "special_field_cols :  ['title' 'authors' 'venue']\n",
      "word_embedding_cols :  ['title' 'authors' 'venue']\n",
      "\n",
      " **** df2 divide columns ****\n",
      "numerical_cols :  ['year']\n",
      "special_field_cols :  ['title' 'authors' 'venue']\n",
      "word_embedding_cols :  ['title' 'authors' 'venue']\n"
     ]
    }
   ],
   "source": [
    "print(\"***preprocessing***\")\n",
    "\n",
    "processor = Preprocessor(special_columns=['title', 'authors', 'venue'])\n",
    "processor.fit(df1_train,df2) #fitting on training dataset for input and on whole dataset for ref\n",
    "\n",
    "processed_train = processor.transform(df1_train, df2)\n",
    "processed_test = processor.transform(df1_test, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_x(processed_data):\n",
    "\n",
    "    '''\n",
    "    get numerical data\n",
    "    '''\n",
    "\n",
    "    num_matrix_1, num_matrix_2 = processed_data[\"numerical\"][0],processed_data[\"numerical\"][1]\n",
    "    embed_matrix_1, embed_matrix_2 = processed_data[\"word_embedding_fields\"][0],processed_data[\"word_embedding_fields\"][1]\n",
    "    spc_matrix_1, spc_matrix_2 = processed_data[\"special_fields\"][0],processed_data[\"special_fields\"][1]\n",
    "\n",
    "    '''\n",
    "    calculate similarities\n",
    "    '''\n",
    "\n",
    "    num_sg_data = similarities().numerical_similarity_on_matrix(num_matrix_1,num_matrix_2,method = \"scaled_gaussian\")\n",
    "    num_mm_data = similarities().numerical_similarity_on_matrix(num_matrix_1,num_matrix_2,method = \"min_max\")\n",
    "    embed_tfidf_data = similarities().vector_similarity_on_matrix(embed_matrix_1,embed_matrix_2)\n",
    "    #embed_mean_data = similarities().vector_similarity_on_matrix(embed_matrix_1,embed_matrix_2)\n",
    "    #embed_min_data = similarities().vector_similarity_on_matrix(embed_matrix_1,embed_matrix_2)\n",
    "    #embed_max_data = similarities().vector_similarity_on_matrix(embed_matrix_1,embed_matrix_2)\n",
    "    spc_lav_data = similarities().text_similarity_on_matrix(spc_matrix_1,spc_matrix_2, method = \"lavenshtein\")\n",
    "    spc_jw_data = similarities().text_similarity_on_matrix(spc_matrix_1, spc_matrix_2, method=\"jaro_winkler\")\n",
    "    spc_jc_data = similarities().text_similarity_on_matrix(spc_matrix_1, spc_matrix_2, method=\"jaccard\")\n",
    "    '''\n",
    "    concatenate all data\n",
    "    '''\n",
    "    # only concatenate non-empty similarity matrices\n",
    "    non_empty = []\n",
    "\n",
    "    for m in num_sg_data, num_mm_data, embed_tfidf_data, spc_lav_data, spc_jw_data, spc_jc_data:\n",
    "        if m.size !=0:\n",
    "            non_empty.append(m)\n",
    "\n",
    "    x = np.concatenate([i for i in non_empty], axis = 1)\n",
    "\n",
    "    print(x.shape)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 14)\n",
      "(12600, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsave features\\n'"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "generate features\n",
    "'''\n",
    "x_train = gen_x(processed_train)\n",
    "x_test = gen_x(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generate labels\n",
    "'''\n",
    "# generate y labels\n",
    "y_train = gen_labels(df1_train_id_col, df2_id_col, match_train, match_id1, match_id2)\n",
    "\n",
    "# simple check to see if x and y match in size\n",
    "print (y_train.shape[0] == x_train.shape[0])\n",
    "print(y_train.sum() == match_train.shape[0])\n",
    "\n",
    "# generate y labels\n",
    "y_test = gen_labels(df1_test_id_col, df2_id_col, match_test, match_id1, match_id2)\n",
    "\n",
    "# simple check to see if x and y match in size\n",
    "print (y_test.shape[0] == x_test.shape[0])\n",
    "print(y_test.sum() == match_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [300], 'max_features': ['sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False], 'class_weight': [None, 'balanced', 'balanced_subsample']}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'class_weight': None, 'bootstrap': False}\n",
      "\tMean CV f1-score : 0.982\n",
      "RF Sample Score (Old Definition)\n",
      "\tPrecision: 0.963\n",
      "\tRecall: 0.981\n",
      "\tF1: 0.972\n",
      "\tAccuracy: 0.9997619047619047\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "modeling\n",
    "'''\n",
    "\n",
    "col_means = np.nanmean(x_train,axis=0)\n",
    "inds_train  = np.where(np.isnan(x_train))\n",
    "inds_test = np.where(np.isnan(x_test))\n",
    "x_train[inds_train]=np.take(col_means, inds_train[1])\n",
    "x_test[inds_test]=np.take(col_means, inds_test[1])\n",
    "\n",
    "# #upsample\n",
    "# x_maj = x_train[y_train==0]\n",
    "# x_min = x_train[y_train==1]\n",
    "# x_min_upsampled = resample(x_min,n_samples=x_maj.shape[0],random_state=42)\n",
    "# x_train_new = np.vstack((x_maj, x_min_upsampled))\n",
    "# y_train_new = np.hstack((np.zeros(x_maj.shape[0]), np.ones(x_maj.shape[0])))\n",
    "\n",
    "# CV\n",
    "# Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "n_estimators=[300]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Class weights for class imbalance issue\n",
    "class_weight = [None, \"balanced\", \"balanced_subsample\"]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'class_weight': class_weight}\n",
    "print(random_grid)\n",
    "# Use the random grid to search for best hyperparameters\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters and use all available cores\n",
    "random_search = RandomizedSearchCV(estimator=rf,\n",
    "                               param_distributions=random_grid,\n",
    "                               n_iter=100,\n",
    "                               cv=3, verbose=2, random_state=42,\n",
    "                               n_jobs=-1, scoring='f1')\n",
    "random_search.fit(x_train, y_train)\n",
    "print(random_search.best_params_)\n",
    "print(\"\\tMean CV f1-score : %1.3f\" % random_search.best_score_ )\n",
    "\n",
    "# fit\n",
    "rf_random = random_search.best_estimator_\n",
    "rf_random.fit(x_train, y_train)\n",
    "# predict\n",
    "y_pred_rf = rf_random.predict(x_test)\n",
    "y_pred_prob_rf = rf_random.predict_proba(x_test)[:, 1]\n",
    "# roc curve\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_prob_rf)\n",
    "# precision, recall, f1\n",
    "print('RF Sample Test-set Score (Old Definition)')\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred_rf))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred_rf))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test, y_pred_rf))\n",
    "print(\"\\tAccuracy: {}\".format(sum(y_pred_rf==y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_orig_labels(pred_df, true_match_df, df1_id_col, match_id1, df2_id_col, match_id2):\n",
    "    \n",
    "    match = true_match_df.dropna()\n",
    "    match['true_label']=1\n",
    "\n",
    "    mat = pred_df[pred_df['prob']>0.5]\n",
    "    pred_match = mat.loc[mat.groupby(match_id1)['prob'].idxmax()]\n",
    "    pred_match['pred_label']=1\n",
    "\n",
    "    id_ = pd.DataFrame(df1_id_col)\n",
    "    id_.columns=[match_id1]\n",
    "\n",
    "    tmp = pred_match.merge(match, how = 'outer')\n",
    "    tmp = tmp.merge(id_,how='outer')\n",
    "    tmp['pred_label'] = tmp['pred_label'].fillna(0).astype(int)\n",
    "    #tmp = tmp.loc[tmp.groupby(match_id1)['pred_label'].idxmin()]\n",
    "\n",
    "    tmp['true_label'] = tmp['true_label'].fillna(0).astype(int)\n",
    "    tmp=tmp.drop(columns=['prob'])\n",
    "    tmp = tmp.sort_values(by=[match_id1])\n",
    "        \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =[]\n",
    "for idx, (id_1,id_2) in enumerate(itertools.product(list(df1_test_id_col), list(df2_id_col))):\n",
    "    res.append([id_1,id_2,y_pred_prob_rf[idx]])\n",
    "\n",
    "assert(len(res) == len(y_pred_prob_rf))    \n",
    "pred_match = pd.DataFrame(res, columns=[match_id1,match_id2,'prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gen_orig_labels(pred_match, match_test, df1_test_id_col, match_id1, df2_id_col, match_id2)\n",
    "true_labels = result['true_label']\n",
    "pred_labels = result['pred_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Sample Test-set Score -- Aligned with Neoway\n",
      "\tPrecision: 0.963\n",
      "\tRecall: 0.981\n",
      "\tF1: 0.972\n"
     ]
    }
   ],
   "source": [
    "print('RF Sample Test-set Score -- Aligned with Neoway')\n",
    "\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(true_labels, pred_labels))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(true_labels, pred_labels))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(true_labels, pred_labels))\n",
    "#print(\"\\tAccuracy: {}\".format(sum(pred==true)/len(true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run best model on full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "read data\n",
    "'''\n",
    "df1 = pd.read_csv(\"data/acm_dblp/full/ACM.csv\", engine='python')\n",
    "df2 = pd.read_csv(\"data/acm_dblp/full/DBLP2.csv\", engine='python')\n",
    "match_df = pd.read_csv(\"data/acm_dblp/full/DBLP-ACM_perfectMapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdf1 train shape:  (1536, 4) \n",
      " \tmatch train shape:  (1483, 2) \n",
      "\tdf1 test shape:  (758, 4) \n",
      "\tmatch test shape:  (741, 2)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "specify id names\n",
    "'''\n",
    "df1_id = 'id'\n",
    "df2_id = 'id'\n",
    "match_id1 = 'idACM' # corresponds to df1_id\n",
    "match_id2 = 'idDBLP' # corresponds to df2_id\n",
    "\n",
    "'''\n",
    "train/test split on input dataset\n",
    "'''\n",
    "#random split inputs into train/test using original dataset\n",
    "df1_train, df1_test = train_test_split(df1, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "#set index dic\n",
    "df1_train_index = dict(zip(df1_train[df1_id], df1_train.reset_index().index))\n",
    "df1_test_index = dict(zip(df1_test[df1_id], df1_test.reset_index().index))\n",
    "\n",
    "'''\n",
    "id column manipulation\n",
    "'''\n",
    "# save for later use to generate labels\n",
    "df1_train_id_col = df1_train[df1_id]\n",
    "df1_test_id_col = df1_test[df1_id]\n",
    "df2_id_col = df2[df2_id]\n",
    "\n",
    "match_train = match_df[match_df['idACM'].isin(df1_train_id_col)]\n",
    "match_test = match_df[match_df['idACM'].isin(df1_test_id_col)]\n",
    "\n",
    "#drop id columns because we don't need to compute id similarity\n",
    "df1_train = df1_train.drop(columns = [df1_id])\n",
    "df1_test = df1_test.drop(columns = [df1_id])\n",
    "df2 = df2.drop(columns = [df2_id])\n",
    "\n",
    "print('\\tdf1 train shape: ', df1_train.shape, '\\n',\n",
    "      '\\tmatch train shape: ', match_train.shape, '\\n'\n",
    "      '\\tdf1 test shape: ', df1_test.shape, '\\n'\n",
    "      '\\tmatch test shape: ', match_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***preprocessing***\n",
      "**** df1 divide columns ****\n",
      "numerical_cols :  ['year']\n",
      "special_field_cols :  ['title' 'authors' 'venue']\n",
      "word_embedding_cols :  ['title' 'authors' 'venue']\n",
      "\n",
      " **** df2 divide columns ****\n",
      "numerical_cols :  ['year']\n",
      "special_field_cols :  ['title' 'authors' 'venue']\n",
      "word_embedding_cols :  ['title' 'authors' 'venue']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print(\"***preprocessing***\")\n",
    "\n",
    "processor = Preprocessor(special_columns=['title', 'authors', 'venue'])\n",
    "processor.fit(df1_train,df2) #fitting on training dataset for input and on whole dataset for ref\n",
    "\n",
    "processed_train = processor.transform(df1_train, df2)\n",
    "processed_test = processor.transform(df1_test, df2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4018176, 14)\n",
      "(1982928, 14)\n",
      "***x_train saved***\n",
      "***x_test saved***\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generate features\n",
    "'''\n",
    "'''\n",
    "x_train = gen_x(processed_train)\n",
    "x_test = gen_x(processed_test)\n",
    "'''\n",
    "'''\n",
    "save features\n",
    "'''\n",
    "'''\n",
    "np.save('acm_dblp_x_train',x_train)\n",
    "#del x_train\n",
    "\n",
    "print(\"***x_train saved***\")\n",
    "\n",
    "np.save('acm_dblp_x_test',x_test)\n",
    "#del x_test\n",
    "\n",
    "print(\"***x_test saved***\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load features\n",
    "'''\n",
    "x_train = np.load('acm_dblp_x_train.npy')\n",
    "print(\"***x_train loaded***\")\n",
    "x_test = np.load('acm_dblp_x_test.npy')\n",
    "print(\"***x_test loaded***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generate labels\n",
    "'''\n",
    "# generate y labels\n",
    "y_train = gen_labels(df1_train_id_col, df2_id_col, match_train, match_id1, match_id2)\n",
    "\n",
    "# simple check to see if x and y match in size\n",
    "print (y_train.shape[0] == x_train.shape[0])\n",
    "print(y_train.sum() == match_train.shape[0])\n",
    "\n",
    "# generate y labels\n",
    "y_test = gen_labels(df1_test_id_col, df2_id_col, match_test, match_id1, match_id2)\n",
    "\n",
    "# simple check to see if x and y match in size\n",
    "print (y_test.shape[0] == x_test.shape[0])\n",
    "print(y_test.sum() == match_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1261\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "#get validation set\n",
    "x_train_new, x_val, y_train_new, y_val = train_test_split(x_train, y_train, stratify=y_train, test_size=0.15, random_state=42)\n",
    "print(sum(y_train_new))\n",
    "print(sum(y_val))\n",
    "del x_train\n",
    "del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute\n",
    "col_means = np.nanmean(x_train_new,axis=0)\n",
    "inds_train  = np.where(np.isnan(x_train_new))\n",
    "inds_val = np.where(np.isnan(x_val))\n",
    "x_train_new[inds_train]=np.take(col_means, inds_train[1])\n",
    "x_val[inds_val]=np.take(col_means, inds_val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score\n",
      "RF\n",
      "\tPrecision: 0.997\n",
      "\tRecall: 0.993\n",
      "\tF1: 0.995\n",
      "\tAccuracy: 0.9999961937654464\n",
      "val score\n",
      "RF\n",
      "\tPrecision: 0.995\n",
      "\tRecall: 0.977\n",
      "\tF1: 0.986\n",
      "\tAccuracy: 0.9999900452443644\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "modeling\n",
    "'''\n",
    "#using hyperparameters found by gridsearch in previous section\n",
    "rf_random = RandomForestClassifier(n_estimators=300,\n",
    "                                   min_samples_split=2,\n",
    "                                   min_samples_leaf=2,\n",
    "                                   max_features='sqrt', max_depth=10,\n",
    "                                   class_weight=None,\n",
    "                                   bootstrap=False, random_state=42,n_jobs=-1)\n",
    "rf_random.fit(x_train_new, y_train_new)\n",
    "\n",
    "print(\"training score\")\n",
    "# predict\n",
    "y_pred_rf = rf_random.predict(x_train_new)\n",
    "y_pred_prob_rf = rf_random.predict_proba(x_train_new)[:, 1]\n",
    "# roc curve\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_train_new, y_pred_prob_rf)\n",
    "# precision, recall, f1\n",
    "print('RF')\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_train_new, y_pred_rf))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_train_new, y_pred_rf))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_train_new, y_pred_rf))\n",
    "print(\"\\tAccuracy: {}\".format(sum(y_pred_rf==y_train_new)/len(y_train_new)))\n",
    "del x_train_new\n",
    "\n",
    "print(\"val score\")\n",
    "# predict\n",
    "y_pred_rf = rf_random.predict(x_val)\n",
    "y_pred_prob_rf = rf_random.predict_proba(x_val)[:, 1]\n",
    "# roc curve\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_val, y_pred_prob_rf)\n",
    "# precision, recall, f1\n",
    "print('RF')\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_val, y_pred_rf))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_val, y_pred_rf))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_val, y_pred_rf))\n",
    "print(\"\\tAccuracy: {}\".format(sum(y_pred_rf==y_val)/len(y_val)))\n",
    "del x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**no overfitting, so evaluate on test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "test score\n",
      "RF\n",
      "\tPrecision: 0.982\n",
      "\tRecall: 0.980\n",
      "\tF1: 0.981\n",
      "\tAccuracy: 0.9999858794671315\n"
     ]
    }
   ],
   "source": [
    "#impute nan's\n",
    "inds_test = np.where(np.isnan(x_test))\n",
    "x_test[inds_test]=np.take(col_means, inds_test[1])\n",
    "\n",
    "print(y_test.shape[0] == x_test.shape[0])\n",
    "\n",
    "print(\"test score\")\n",
    "# predict\n",
    "y_pred_rf = rf_random.predict(x_test)\n",
    "y_pred_prob_rf = rf_random.predict_proba(x_test)[:, 1]\n",
    "# roc curve\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_prob_rf)\n",
    "# precision, recall, f1\n",
    "print('RF')\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred_rf))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred_rf))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(y_test, y_pred_rf))\n",
    "print(\"\\tAccuracy: {}\".format(sum(y_pred_rf==y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of true matches\n",
      "741\n"
     ]
    }
   ],
   "source": [
    "print(\"num of true matches\")\n",
    "print(sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of predicted matches\n",
      "739\n"
     ]
    }
   ],
   "source": [
    "print(\"num of predicted matches\")\n",
    "print(sum(y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =[]\n",
    "for idx, (id_1,id_2) in enumerate(itertools.product(list(df1_test_id_col), list(df2_id_col))):\n",
    "    res.append([id_1,id_2,y_pred_prob_rf[idx]])\n",
    "pred_match = pd.DataFrame(res, columns=[match_id1,match_id2,'prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gen_orig_labels(pred_match, match_test,df1_test_id_col, match_id1, df2_id_col, match_id2)\n",
    "true = result['true_label']\n",
    "pred = result['pred_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Full Data Test-set Score -- Aligned with Neoway\n",
      "\tPrecision: 0.992\n",
      "\tRecall: 0.976\n",
      "\tF1: 0.984\n",
      "\tAccuracy: 0.9685452162516383\n"
     ]
    }
   ],
   "source": [
    "print('RF Full Data Test-set Score -- Aligned with Neoway')\n",
    "\n",
    "print(\"\\tPrecision: %1.3f\" % precision_score(true, pred))\n",
    "print(\"\\tRecall: %1.3f\" % recall_score(true, pred))\n",
    "print(\"\\tF1: %1.3f\" % f1_score(true, pred))\n",
    "print(\"\\tAccuracy: {}\".format(sum(pred==true)/len(true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***model saved***\n"
     ]
    }
   ],
   "source": [
    "# save the classifier\n",
    "'''\n",
    "import pickle\n",
    "with open('acm_dblp_rf.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf_random, fid, protocol=4)\n",
    "\n",
    "print(\"***model saved***\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
