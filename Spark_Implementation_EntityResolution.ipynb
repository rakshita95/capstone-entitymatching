{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark implementation of Entity Resolution System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for productionalizing our system in Spark. The code allows to distribute the preprocessing stage and the prediction stage. For training one can train the model locally and load it into this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import jellyfish as jf\n",
    "import scipy.spatial.distance as dist\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, col, lit, udf, struct, concat_ws, collect_list, explode, split\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from modules.spark.process_text import Process_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "data1 = \"/data/input.csv\"\n",
    "data2 = \"/data/reference.csv\"\n",
    "\n",
    "path = \"worddict/GoogleNews-vectors-negative300.bin\"\n",
    "df_1 = spark.read.csv(data1, header=True)\n",
    "df_2 = spark.read.csv(data2, header=True)\n",
    "test = \"/data/test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec(path, words):\n",
    "    \"\"\" creates a word to vec dictionary only on the words present in the corpus\n",
    "    \"\"\"\n",
    "    word_dict = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    vectors = {}\n",
    "    for w in words: \n",
    "        if w not in word_dict:\n",
    "            continue\n",
    "        else:\n",
    "            vectors[w] = word_dict[w]\n",
    "    return vectors\n",
    "\n",
    "def clean_text(x):\n",
    "    text_processor = Process_text()\n",
    "    processed_sentence = nltk.word_tokenize(unicode(x))\n",
    "    processed_sentence = text_processor.remove_non_ascii(processed_sentence)\n",
    "    processed_sentence = text_processor.to_lowercase(processed_sentence)\n",
    "    processed_sentence = text_processor.remove_punctuation(processed_sentence)\n",
    "    processed_sentence = text_processor.remove_nan(processed_sentence)\n",
    "    processed_sentence = text_processor.remove_stopwords(processed_sentence)\n",
    "    if processed_sentence: \n",
    "        return processed_sentence\n",
    "    else:\n",
    "        return ['None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide columns: define which columns belong into which category\n",
    "special_columns = ['addressStreet', 'name', 'addressCity', 'addressZip']\n",
    "word_columns = ['name', 'addressStreet', 'addressCity', 'addressState']\n",
    "numeric_columns = ['addressZip']\n",
    "\n",
    "# get the unique words in the dataset\n",
    "df1 = df_1.withColumn('wordemb', concat_ws(' ', *word_columns))\n",
    "df2 = df_2.withColumn('wordemb', concat_ws(' ', *word_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get unique words\n",
    "words1 = df1.withColumn('word', explode(split(col('wordemb'), ' '))).select('word').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "words2 = df2.withColumn('word', explode(split(col('wordemb'), ' '))).select('word').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "#reduce the lookup dictionary to the words relevant in the text\n",
    "wo = []\n",
    "for i in words1 + words2: \n",
    "    wo.append(clean_text(i)[0])\n",
    "word_dict = word2vec(path, wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word2vec(sentence, word_dict=word_dict, word_dict_size=300):\n",
    "    l = np.zeros(word_dict_size)\n",
    "    i = 0\n",
    "    for w in sentence:\n",
    "        if w not in word_dict.keys():\n",
    "            continue\n",
    "        else:\n",
    "            l += word_dict[w]\n",
    "            i += 1\n",
    "\n",
    "    if i == 0: \n",
    "        i = 1\n",
    "    return l / i\n",
    "\n",
    "def pptext(x):    \n",
    "    processed_sentence = clean_text(x)\n",
    "    return get_word2vec(processed_sentence).tolist()\n",
    "    \n",
    "@udf('string')\n",
    "def geo_code_address(x, api_key='api_key'):\n",
    "    if not(api_key):\n",
    "        print(\"Geocoding Failed! Please provide an API key\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if type(x) == list or type(x) == np.ndarray or type(x) == pd.core.series.Series:\n",
    "        address = ', '.join(x)\n",
    "    else:\n",
    "        address = x\n",
    "    api = GoogleV3(api_key=api_key)\n",
    "    loc = api.geocode(address)\n",
    "    if loc:\n",
    "        return loc.address, loc.latitude, loc.longitude\n",
    "    else:\n",
    "        return str(None, None, None)\n",
    "\n",
    "def levenshtein(a,b):\n",
    "    tmp = [levenshtein_distance(x, y) for i, x in enumerate(a) for j, y in enumerate(b) if i == j]\n",
    "    return np.asarray(tmp)\n",
    "\n",
    "\n",
    "@udf('string')\n",
    "def vector_similarity(x):\n",
    "    return dist.cdist([x[0]],[x[1]], 'cosine').tolist()[0][0]\n",
    "        \n",
    "    \n",
    "\n",
    "@udf('string')\n",
    "def numeric_similarity(x):\n",
    "    return float(np.exp(-2 * abs(float(x[0]) - float(x[1])) / (float(x[0]) + float(x[1]) + 1e-5)))\n",
    "\n",
    "\n",
    "@udf('string')\n",
    "def string_similarity(x):\n",
    "    return jf.levenshtein_distance(unicode(x[0]),unicode(x[1]))\n",
    "\n",
    "@udf('string')\n",
    "def phone_number(x, broadcast1):\n",
    "    # apply word embeddings to the line \n",
    "    return y\n",
    "\n",
    "@udf('string')\n",
    "def letter_count(x):\n",
    "    processed_sentence = nltk.word_tokenize(sentence)\n",
    "    return len(process_sentence)\n",
    "\n",
    "#preprocess_text = udf(lambda x: pptext(x), StringType())\n",
    "\n",
    "@udf('string')\n",
    "def preprocess_text(x,word_dict=word_dict):\n",
    "    y = pptext(x,word_dict)\n",
    "    return y\n",
    "\n",
    "prep = udf(pptext, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the word embeddings \n",
    "for i in word_columns:\n",
    "    df1 = df1.withColumn('wordemb_'+str(i), prep(col(i)))\n",
    "    df2 = df2.withColumn('wordemb_'+str(i), prep(col(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df2.selectExpr(*[\"{} as {}_2\".format(i,i) for i in df2.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfx = df1.crossJoin(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- serial: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- addressStreet: string (nullable = true)\n",
      " |-- addressCity: string (nullable = true)\n",
      " |-- addressZip: string (nullable = true)\n",
      " |-- addressState: string (nullable = true)\n",
      " |-- wordemb: string (nullable = false)\n",
      " |-- wordemb_name: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- wordemb_addressStreet: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- wordemb_addressCity: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- wordemb_addressState: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- serial_2: string (nullable = true)\n",
      " |-- name_2: string (nullable = true)\n",
      " |-- addressStreet_2: string (nullable = true)\n",
      " |-- addressCity_2: string (nullable = true)\n",
      " |-- addressZip_2: string (nullable = true)\n",
      " |-- addressState_2: string (nullable = true)\n",
      " |-- wordemb_2: string (nullable = false)\n",
      " |-- wordemb_name_2: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- wordemb_addressStreet_2: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- wordemb_addressCity_2: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- wordemb_addressState_2: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfx.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+--------------------+-----------+----------+------------+--------------------+--------------------+---------------------+--------------------+--------------------+--------+-------------------+---------------+-------------+------------+--------------+--------------------+--------------------+-----------------------+---------------------+----------------------+\n",
      "|serial|             name|       addressStreet|addressCity|addressZip|addressState|             wordemb|        wordemb_name|wordemb_addressStreet| wordemb_addressCity|wordemb_addressState|serial_2|             name_2|addressStreet_2|addressCity_2|addressZip_2|addressState_2|           wordemb_2|      wordemb_name_2|wordemb_addressStreet_2|wordemb_addressCity_2|wordemb_addressState_2|\n",
      "+------+-----------------+--------------------+-----------+----------+------------+--------------------+--------------------+---------------------+--------------------+--------------------+--------+-------------------+---------------+-------------+------------+--------------+--------------------+--------------------+-----------------------+---------------------+----------------------+\n",
      "|   201|HONG KONG EXPRESS|11252 S NORMANDIE...|LOS ANGELES|     90044|          CA|HONG KONG EXPRESS...|[0.0033365886, 0....| [-0.080078125, 0....|[-0.13635254, 0.0...|[-0.018066406, 0....|   15928|BROOKDALE RADIATION|1 BROOKDALE PLZ|     BROOKLYN|       11212|            NY|BROOKDALE RADIATI...|[-0.09375, -0.031...|   [-0.022216797, 0....| [-0.3046875, -0.1...|  [-0.17578125, 2.1...|\n",
      "|   619|       PETE'S MKT|    9141 S HOOVER ST|LOS ANGELES|     90044|          CA|PETE'S MKT 9141 S...|[-0.11010742, 0.1...| [-0.043762207, 0....|[-0.13635254, 0.0...|[-0.018066406, 0....|   15928|BROOKDALE RADIATION|1 BROOKDALE PLZ|     BROOKLYN|       11212|            NY|BROOKDALE RADIATI...|[-0.09375, -0.031...|   [-0.022216797, 0....| [-0.3046875, -0.1...|  [-0.17578125, 2.1...|\n",
      "+------+-----------------+--------------------+-----------+----------+------------+--------------------+--------------------+---------------------+--------------------+--------------------+--------+-------------------+---------------+-------------+------------+--------------+--------------------+--------------------+-----------------------+---------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfx.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "colnames = ['serial','serial_2']\n",
    "for s in word_columns:\n",
    "    col_name = 'wordemb'+str(i)\n",
    "    dfx = dfx.withColumn(col_name, vector_similarity(struct(*['wordemb_{}'.format(s), 'wordemb_{}_2'.format(s)])))\n",
    "    colnames.append(col_name)\n",
    "    i += 1\n",
    "\n",
    "for s in special_columns:\n",
    "    col_name = 'stringsim_'+str(i)\n",
    "    dfx = dfx.withColumn(col_name, string_similarity(struct(*['{}'.format(s), '{}_2'.format(s)])))\n",
    "    colnames.append(col_name)\n",
    "    i += 1\n",
    "\n",
    "for s in numeric_columns:\n",
    "    col_name = 'numericsim_'+str(i)\n",
    "    dfx = dfx.withColumn(col_name, numeric_similarity(struct(*['{}'.format(s), '{}_2'.format(s)])))\n",
    "    colnames.append(col_name)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------------+------------------+-----------------+--------------------+-----------+-----------+-----------+-----------+-------------------+\n",
      "|serial|serial_2|          wordemb0|          wordemb1|         wordemb2|            wordemb3|stringsim_4|stringsim_5|stringsim_6|stringsim_7|       numericsim_8|\n",
      "+------+--------+------------------+------------------+-----------------+--------------------+-----------+-----------+-----------+-----------+-------------------+\n",
      "|   201|   15928|0.9361319069348657|0.6553759142709283|0.623078322225926|  0.5531163635847764|         15|         18|         10|          5|0.21074992522577732|\n",
      "|   619|   15928|0.9354736562021811|0.6997708710444316|0.623078322225926|  0.5531163635847764|         14|         17|         10|          5|0.21074992522577732|\n",
      "|   763|   15928|0.9624305454289781|0.7266836075336558|              0.0|1.110223024625156...|         14|         17|          0|          0|                1.0|\n",
      "|  1054|   15928|0.8395923257860176|0.5706139105431893|0.623078322225926|  0.5531163635847764|         15|         15|         10|          5|0.21074992522577732|\n",
      "|  1360|   15928|0.9102206708036737|0.5059458113149138|0.623078322225926|  0.5531163635847764|         17|         14|         10|          5|0.21074992522577732|\n",
      "|  1701|   15928|0.9254250138921126|0.6843149523883647|              0.0|1.110223024625156...|         13|         18|          0|          0|                1.0|\n",
      "|  1766|   15928|               NaN|0.5706139105431893|0.623078322225926|  0.5531163635847764|         16|         16|         10|          5|0.21074992522577732|\n",
      "|  1837|   15928| 1.070102569184367|0.6553759142709283|0.623078322225926|  0.5531163635847764|         15|         14|         10|          5|0.21074992522577732|\n",
      "|  1843|   15928|0.8920219146112169|0.6553759142709283|              0.0|1.110223024625156...|         13|         23|          0|          0|                1.0|\n",
      "|  2007|   15928| 0.844013525809838|0.6792465124336791|0.623078322225926|  0.5531163635847764|         14|         18|         10|          4|0.20955213481378013|\n",
      "+------+--------+------------------+------------------+-----------------+--------------------+-----------+-----------+-----------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfx.select(colnames).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parellelize the Machine Learning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load the ML model from sklearn\n",
    "\n",
    "# To load it, all packages need to be the following specifications: \n",
    "# numpy==1.13.3 sklearn==0.19.1 scipy==0.19.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_random = pickle.load(open('rf_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dummy model \n",
    "\n",
    "#x_train = [[15,10,5,0.45756],[13,0,0,0.40601]]\n",
    "#y_train = [0,1]\n",
    "\n",
    "#rf_random = RandomForestClassifier(random_state=40)\n",
    "#rf_fit = rf_random.fit(x_train,y_train)\n",
    "#y_pred_prob_rf = rf_random.predict_proba(np.array([[4,3,2,5],[5,6,4,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write predict function as udf\n",
    "@udf('string')\n",
    "def make_prediction(x, model=rf_random):\n",
    "    proba = rf_random.predict_proba([x])\n",
    "    return proba.tolist()[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply udf do the dataframe\n",
    "dfx = dfx.select(colnames).withColumn('proba', make_prediction(struct(*colnames[2:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----+----+-------------------+-----+\n",
      "|serial|serial_2|sim0|sim1|sim2|               sim3|proba|\n",
      "+------+--------+----+----+----+-------------------+-----+\n",
      "|   201|   15928|  15|  10|   5| 0.4575667877711177|  0.5|\n",
      "|   619|   15928|  14|  10|   5| 0.3667975146235235|  0.5|\n",
      "|   763|   15928|  14|   0|   0| 0.2946390329558247|  0.5|\n",
      "|  1054|   15928|  15|  10|   5| 0.4244207038629879|  0.5|\n",
      "|  1360|   15928|  17|  10|   5|0.39429010305496726|  0.5|\n",
      "|  1701|   15928|  13|   0|   0|0.45274087705598687|  0.5|\n",
      "|  1766|   15928|  16|  10|   5|0.42584238257035933|  0.5|\n",
      "|  1837|   15928|  15|  10|   5| 0.5027467177081633|  0.5|\n",
      "|  1843|   15928|  13|   0|   0| 0.4060131115143911|  0.5|\n",
      "|  2007|   15928|  14|  10|   4| 0.5019455298693172|  0.5|\n",
      "|  2096|   15928|  15|  10|   4| 0.4940773373093259|  0.5|\n",
      "|  2317|   15928|  16|  10|   5| 0.4159686918959705|  0.5|\n",
      "|  2407|   15928|  11|   0|   0|0.27789604104707744|  0.5|\n",
      "|  2442|   15928|  15|  10|   4| 0.5325185882400332|  0.5|\n",
      "|  2553|   15928|  15|  10|   5| 0.5029624085859838|  0.5|\n",
      "|  3044|   15928|  14|   0|   0|0.36050532346950626|  0.5|\n",
      "|  3079|   15928|  16|  10|   4| 0.4467687194113886|  0.5|\n",
      "|  3426|   15928|  16|   0|   0|0.33061273664764335|  0.5|\n",
      "|  3485|   15928|  15|  10|   5|0.42263007442158906|  0.5|\n",
      "|  3607|   15928|  14|  10|   5| 0.4225939365500042|  0.5|\n",
      "+------+--------+----+----+----+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfx.select(show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----+----+-------------------+-----+---+\n",
      "|serial|serial_2|sim0|sim1|sim2|               sim3|proba|max|\n",
      "+------+--------+----+----+----+-------------------+-----+---+\n",
      "|   201|  512983|  18|  10|   5| 0.3929147390570289|  0.5|0.5|\n",
      "|   619|  512983|   9|  10|   5| 0.2781830373639601|  0.5|0.5|\n",
      "|   763|  512983|  12|   0|   0|0.19465777771881887|  0.5|0.5|\n",
      "|  1054|  512983|  15|  10|   5| 0.3216807467773285|  0.5|0.5|\n",
      "|  1360|  512983|  12|  10|   5|0.28683031486913846|  0.5|0.5|\n",
      "+------+--------+----+----+----+-------------------+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select the highest value per predict (group by, select max)\n",
    "w = Window.partitionBy('serial_2')\n",
    "dfx.withColumn('max', F.max('proba').over(w))\\\n",
    "    .where(col('proba') == col('max'))\\\n",
    "    .select(['serial','serial_2']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the matching indices\n",
    "dfx.select(['serial','serial_2'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
