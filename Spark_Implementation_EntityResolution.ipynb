{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark implementation of Entity Resolution System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for productionalizing our system in Spark. The code allows to distribute the preprocessing stage and the prediction stage. For training one can train the model locally and load it into this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import jellyfish as jf\n",
    "import scipy.spatial.distance as dist\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, col, lit, udf, struct, concat_ws, collect_list, explode, split\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from modules.spark.process_text import Process_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "data1 = \"/data/input.csv\"\n",
    "data2 = \"/data/reference.csv\"\n",
    "\n",
    "path = \"worddict/GoogleNews-vectors-negative300.bin\"\n",
    "df_1 = spark.read.csv(data1, header=True)\n",
    "df_2 = spark.read.csv(data2, header=True)\n",
    "test = \"/data/test.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec(path, words):\n",
    "    \"\"\" creates a word to vec dictionary only on the words present in the corpus\n",
    "    \"\"\"\n",
    "    word_dict = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    vectors = {}\n",
    "    for w in words: \n",
    "        if w not in word_dict:\n",
    "            continue\n",
    "        else:\n",
    "            vectors[w] = word_dict[w]\n",
    "    return vectors\n",
    "\n",
    "def clean_text(x):\n",
    "    text_processor = Process_text()\n",
    "    processed_sentence = nltk.word_tokenize(unicode(x))\n",
    "    processed_sentence = text_processor.remove_non_ascii(processed_sentence)\n",
    "    processed_sentence = text_processor.to_lowercase(processed_sentence)\n",
    "    processed_sentence = text_processor.remove_punctuation(processed_sentence)\n",
    "    processed_sentence = text_processor.remove_nan(processed_sentence)\n",
    "    processed_sentence = text_processor.remove_stopwords(processed_sentence)\n",
    "    if processed_sentence: \n",
    "        return processed_sentence\n",
    "    else:\n",
    "        return ['None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide columns: define which columns belong into which category\n",
    "special_columns = [\"addressStreet\", \"addressCity\", \"addressZIP\"]\n",
    "word_columns = [item[0] for item in df_1.dtypes if item[1].startswith('string')]\n",
    "numeric_columns = [item[0] for item in df_1.dtypes if item[1].startswith('float')]\n",
    "\n",
    "# get the unique words in the dataset\n",
    "df1 = df_1.withColumn('wordemb', concat_ws(' ', *word_columns))\n",
    "df2 = df_2.withColumn('wordemb', concat_ws(' ', *word_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get unique words\n",
    "words1 = df1.withColumn('word', explode(split(col('wordemb'), ' '))).select('word').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "words2 = df2.withColumn('word', explode(split(col('wordemb'), ' '))).select('word').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "#reduce the lookup dictionary to the words relevant in the text\n",
    "wo = []\n",
    "for i in words1 + words2: \n",
    "    wo.append(clean_text(i)[0])\n",
    "word_dict = word2vec(path, wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word2vec(sentence, word_dict=word_dict, word_dict_size=300):\n",
    "    l = np.zeros(word_dict_size)\n",
    "    i = 0\n",
    "    for w in sentence:\n",
    "        if w not in word_dict.keys():\n",
    "            continue\n",
    "        else:\n",
    "            l += word_dict[w]\n",
    "            i += 1\n",
    "\n",
    "    if i == 0: \n",
    "        i = 1\n",
    "    return l / i\n",
    "\n",
    "def pptext(x):    \n",
    "    processed_sentence = clean_text(x)\n",
    "    return get_word2vec(processed_sentence).tolist()\n",
    "    \n",
    "@udf('string')\n",
    "def geo_code_address(x, api_key='api_key'):\n",
    "    if not(api_key):\n",
    "        print(\"Geocoding Failed! Please provide an API key\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if type(x) == list or type(x) == np.ndarray or type(x) == pd.core.series.Series:\n",
    "        address = ', '.join(x)\n",
    "    else:\n",
    "        address = x\n",
    "    api = GoogleV3(api_key=api_key)\n",
    "    loc = api.geocode(address)\n",
    "    if loc:\n",
    "        return loc.address, loc.latitude, loc.longitude\n",
    "    else:\n",
    "        return str(None, None, None)\n",
    "\n",
    "def levenshtein(a,b):\n",
    "    tmp = [levenshtein_distance(x, y) for i, x in enumerate(a) for j, y in enumerate(b) if i == j]\n",
    "    return np.asarray(tmp)\n",
    "\n",
    "\n",
    "@udf('string')\n",
    "def vector_similarity(x):\n",
    "    return dist.cdist([x[0]],[x[1]], 'cosine').tolist()[0][0]\n",
    "        \n",
    "    \n",
    "\n",
    "@udf('string')\n",
    "def numeric_similarity(x):\n",
    "    return float(np.exp(-2 * abs(float(x[0]) - float(x[1])) / (float(x[0]) + float(x[1]) + 1e-5)))\n",
    "\n",
    "\n",
    "@udf('string')\n",
    "def string_similarity(x):\n",
    "    return jf.levenshtein_distance(unicode(x[0]),unicode(x[1]))\n",
    "\n",
    "@udf('string')\n",
    "def phone_number(x, broadcast1):\n",
    "    # apply word embeddings to the line \n",
    "    return y\n",
    "\n",
    "@udf('string')\n",
    "def letter_count(x):\n",
    "    processed_sentence = nltk.word_tokenize(sentence)\n",
    "    return len(process_sentence)\n",
    "\n",
    "#preprocess_text = udf(lambda x: pptext(x), StringType())\n",
    "\n",
    "@udf('string')\n",
    "def preprocess_text(x,word_dict=word_dict):\n",
    "    y = pptext(x,word_dict)\n",
    "    return y\n",
    "\n",
    "prep = udf(pptext, ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve '`wordemb`' given input columns: [name_2, addressCity_2, wordemb_2, addressZip_2, addressStreet_2, serial_2, addressState_2];;\\n'Project [serial_2#112, name_2#113, addressStreet_2#114, addressCity_2#115, addressZip_2#116, addressState_2#117, wordemb_2#118, pptext('wordemb) AS wordemb#371]\\n+- AnalysisBarrier\\n      +- Project [serial#32 AS serial_2#112, name#33 AS name_2#113, addressStreet#34 AS addressStreet_2#114, addressCity#35 AS addressCity_2#115, addressZip#36 AS addressZip_2#116, addressState#37 AS addressState_2#117, wordemb#104 AS wordemb_2#118]\\n         +- Project [serial#32, name#33, addressStreet#34, addressCity#35, addressZip#36, addressState#37, pptext(wordemb#52) AS wordemb#104]\\n            +- Project [serial#32, name#33, addressStreet#34, addressCity#35, addressZip#36, addressState#37, concat_ws( , serial#32, name#33, addressStreet#34, addressCity#35, addressZip#36, addressState#37) AS wordemb#52]\\n               +- Relation[serial#32,name#33,addressStreet#34,addressCity#35,addressZip#36,addressState#37] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4a19dee64be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordemb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordemb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordemb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordemb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \"\"\"\n\u001b[1;32m   1848\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve '`wordemb`' given input columns: [name_2, addressCity_2, wordemb_2, addressZip_2, addressStreet_2, serial_2, addressState_2];;\\n'Project [serial_2#112, name_2#113, addressStreet_2#114, addressCity_2#115, addressZip_2#116, addressState_2#117, wordemb_2#118, pptext('wordemb) AS wordemb#371]\\n+- AnalysisBarrier\\n      +- Project [serial#32 AS serial_2#112, name#33 AS name_2#113, addressStreet#34 AS addressStreet_2#114, addressCity#35 AS addressCity_2#115, addressZip#36 AS addressZip_2#116, addressState#37 AS addressState_2#117, wordemb#104 AS wordemb_2#118]\\n         +- Project [serial#32, name#33, addressStreet#34, addressCity#35, addressZip#36, addressState#37, pptext(wordemb#52) AS wordemb#104]\\n            +- Project [serial#32, name#33, addressStreet#34, addressCity#35, addressZip#36, addressState#37, concat_ws( , serial#32, name#33, addressStreet#34, addressCity#35, addressZip#36, addressState#37) AS wordemb#52]\\n               +- Relation[serial#32,name#33,addressStreet#34,addressCity#35,addressZip#36,addressState#37] csv\\n\""
     ]
    }
   ],
   "source": [
    "# Get the word embeddings \n",
    "df1 = df1.withColumn('wordemb', prep(col('wordemb')))\n",
    "df2 = df2.withColumn('wordemb', prep(col('wordemb')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df2.selectExpr(*[\"{} as {}_2\".format(i,i) for i in df2.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfx = df1.crossJoin(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- serial: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- addressStreet: string (nullable = true)\n",
      " |-- addressCity: string (nullable = true)\n",
      " |-- addressZip: string (nullable = true)\n",
      " |-- addressState: string (nullable = true)\n",
      " |-- wordemb: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- serial_2: string (nullable = true)\n",
      " |-- name_2: string (nullable = true)\n",
      " |-- addressStreet_2: string (nullable = true)\n",
      " |-- addressCity_2: string (nullable = true)\n",
      " |-- addressZip_2: string (nullable = true)\n",
      " |-- addressState_2: string (nullable = true)\n",
      " |-- wordemb_2: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfx.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+--------------------+-----------+----------+------------+--------------------+--------+-------------------+---------------+-------------+------------+--------------+--------------------+\n",
      "|serial|             name|       addressStreet|addressCity|addressZip|addressState|             wordemb|serial_2|             name_2|addressStreet_2|addressCity_2|addressZip_2|addressState_2|           wordemb_2|\n",
      "+------+-----------------+--------------------+-----------+----------+------------+--------------------+--------+-------------------+---------------+-------------+------------+--------------+--------------------+\n",
      "|   201|HONG KONG EXPRESS|11252 S NORMANDIE...|LOS ANGELES|     90044|          CA|[-0.051548548, 0....|   15928|BROOKDALE RADIATION|1 BROOKDALE PLZ|     BROOKLYN|       11212|            NY|[-0.123730466, 0....|\n",
      "|   619|       PETE'S MKT|    9141 S HOOVER ST|LOS ANGELES|     90044|          CA|[-0.08550154, 0.1...|   15928|BROOKDALE RADIATION|1 BROOKDALE PLZ|     BROOKLYN|       11212|            NY|[-0.123730466, 0....|\n",
      "+------+-----------------+--------------------+-----------+----------+------------+--------------------+--------+-------------------+---------------+-------------+------------+--------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfx.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "colnames = ['serial','serial_2']\n",
    "for s in special_columns:\n",
    "    col_name = 'sim'+str(i)\n",
    "    dfx = dfx.withColumn(col_name, string_similarity(struct(*['{}'.format(s), '{}_2'.format(s)])))\n",
    "    colnames.append(col_name)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in numeric_columns:\n",
    "    col_name = 'sim'+str(i)\n",
    "    dfx = dfx.withColumn(col_name, numeric_similarity(struct(*['{}'.format(s), '{}_2'.format(s)])))\n",
    "    colnames.append(col_name)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in ['wordemb']:\n",
    "    col_name = 'sim'+str(i)\n",
    "    dfx = dfx.withColumn(col_name, vector_similarity(struct(*['{}'.format(s), '{}_2'.format(s)])))\n",
    "    colnames.append(col_name)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----+----+----+-------------------+----+\n",
      "|serial|serial_2|sim0|sim1|sim2|sim3|               sim4|sim5|\n",
      "+------+--------+----+----+----+----+-------------------+----+\n",
      "|   201|   15928|  18|  15|  10|   5|0.21074992522577732| NaN|\n",
      "|   619|   15928|  17|  14|  10|   5|0.21074992522577732| NaN|\n",
      "|   763|   15928|  17|  14|   0|   0|                1.0| NaN|\n",
      "|  1054|   15928|  15|  15|  10|   5|0.21074992522577732| NaN|\n",
      "|  1360|   15928|  14|  17|  10|   5|0.21074992522577732| NaN|\n",
      "|  1701|   15928|  18|  13|   0|   0|                1.0| NaN|\n",
      "|  1766|   15928|  16|  16|  10|   5|0.21074992522577732| NaN|\n",
      "|  1837|   15928|  14|  15|  10|   5|0.21074992522577732| NaN|\n",
      "|  1843|   15928|  23|  13|   0|   0|                1.0| NaN|\n",
      "|  2007|   15928|  18|  14|  10|   4|0.20955213481378013| NaN|\n",
      "|  2096|   15928|  14|  15|  10|   4|0.20955213481378013| NaN|\n",
      "|  2317|   15928|  14|  16|  10|   5|0.21074992522577732| NaN|\n",
      "|  2407|   15928|  14|  11|   0|   0|                1.0| NaN|\n",
      "|  2442|   15928|  16|  15|  10|   4|0.20955213481378013| NaN|\n",
      "|  2553|   15928|  14|  15|  10|   5|0.21074992522577732| NaN|\n",
      "|  3044|   15928|  16|  14|   0|   0|                1.0| NaN|\n",
      "|  3079|   15928|  16|  16|  10|   4|0.20955213481378013| NaN|\n",
      "|  3426|   15928|  35|  16|   0|   0|                1.0| NaN|\n",
      "|  3485|   15928|  17|  15|  10|   5|0.21074992522577732| NaN|\n",
      "|  3607|   15928|  17|  14|  10|   5|0.21074992522577732| NaN|\n",
      "+------+--------+----+----+----+----+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfx.select(colnames).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parellelize the Machine Learning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load the ML model from sklearn\n",
    "\n",
    "# To load it, all packages need to be the following specifications: \n",
    "# numpy==1.13.3 sklearn==0.19.1 scipy==0.19.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_random = pickle.load(open('rf_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get a dummy model \n",
    "\n",
    "#x_train = [[15,10,5,0.45756],[13,0,0,0.40601]]\n",
    "#y_train = [0,1]\n",
    "\n",
    "#rf_random = RandomForestClassifier(random_state=40)\n",
    "#rf_fit = rf_random.fit(x_train,y_train)\n",
    "#y_pred_prob_rf = rf_random.predict_proba(np.array([[4,3,2,5],[5,6,4,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write predict function as udf\n",
    "@udf('string')\n",
    "def make_prediction(x, model=rf_random):\n",
    "    proba = rf_random.predict_proba([x])\n",
    "    return proba.tolist()[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply udf do the dataframe\n",
    "dfx = dfx.select(colnames).withColumn('proba', make_prediction(struct(*colnames[2:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----+----+-------------------+-----+\n",
      "|serial|serial_2|sim0|sim1|sim2|               sim3|proba|\n",
      "+------+--------+----+----+----+-------------------+-----+\n",
      "|   201|   15928|  15|  10|   5| 0.4575667877711177|  0.5|\n",
      "|   619|   15928|  14|  10|   5| 0.3667975146235235|  0.5|\n",
      "|   763|   15928|  14|   0|   0| 0.2946390329558247|  0.5|\n",
      "|  1054|   15928|  15|  10|   5| 0.4244207038629879|  0.5|\n",
      "|  1360|   15928|  17|  10|   5|0.39429010305496726|  0.5|\n",
      "|  1701|   15928|  13|   0|   0|0.45274087705598687|  0.5|\n",
      "|  1766|   15928|  16|  10|   5|0.42584238257035933|  0.5|\n",
      "|  1837|   15928|  15|  10|   5| 0.5027467177081633|  0.5|\n",
      "|  1843|   15928|  13|   0|   0| 0.4060131115143911|  0.5|\n",
      "|  2007|   15928|  14|  10|   4| 0.5019455298693172|  0.5|\n",
      "|  2096|   15928|  15|  10|   4| 0.4940773373093259|  0.5|\n",
      "|  2317|   15928|  16|  10|   5| 0.4159686918959705|  0.5|\n",
      "|  2407|   15928|  11|   0|   0|0.27789604104707744|  0.5|\n",
      "|  2442|   15928|  15|  10|   4| 0.5325185882400332|  0.5|\n",
      "|  2553|   15928|  15|  10|   5| 0.5029624085859838|  0.5|\n",
      "|  3044|   15928|  14|   0|   0|0.36050532346950626|  0.5|\n",
      "|  3079|   15928|  16|  10|   4| 0.4467687194113886|  0.5|\n",
      "|  3426|   15928|  16|   0|   0|0.33061273664764335|  0.5|\n",
      "|  3485|   15928|  15|  10|   5|0.42263007442158906|  0.5|\n",
      "|  3607|   15928|  14|  10|   5| 0.4225939365500042|  0.5|\n",
      "+------+--------+----+----+----+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfx.select(show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+----+----+-------------------+-----+---+\n",
      "|serial|serial_2|sim0|sim1|sim2|               sim3|proba|max|\n",
      "+------+--------+----+----+----+-------------------+-----+---+\n",
      "|   201|  512983|  18|  10|   5| 0.3929147390570289|  0.5|0.5|\n",
      "|   619|  512983|   9|  10|   5| 0.2781830373639601|  0.5|0.5|\n",
      "|   763|  512983|  12|   0|   0|0.19465777771881887|  0.5|0.5|\n",
      "|  1054|  512983|  15|  10|   5| 0.3216807467773285|  0.5|0.5|\n",
      "|  1360|  512983|  12|  10|   5|0.28683031486913846|  0.5|0.5|\n",
      "+------+--------+----+----+----+-------------------+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select the highest value per predict (group by, select max)\n",
    "w = Window.partitionBy('serial_2')\n",
    "dfx.withColumn('max', F.max('proba').over(w))\\\n",
    "    .where(col('proba') == col('max'))\\\n",
    "    .select(['serial','serial_2']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the matching indices\n",
    "dfx.select(['serial','serial_2'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
